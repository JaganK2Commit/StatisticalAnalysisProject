---
title: "Data Analysis Project"
author: "STAT 420, Summer 2021, Jagadeesh Kedarisetty, Nilesh Bhandarwar, Peri Rocha"
date: "08/08/2021"
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

**-- TODO: WORK TO BE DONE. CLEAN THIS UP --**

1. Describe the project goal
2. Description of the original data file including description of all relevant variables
3. Brief the structure of dataset, number of observations, number of predictors, numerical, categorical. 
4. Data analysis techniques and concepts that are utilized in the project
  - Data cleaning
  - Collinearity
  - Multiple linear regression
  - ANOVA
  - Interaction
  - Assumption diagnostics
  - Outlier diagnostics
  - Transformations
  - Polynomial regression
  - Stepwise model selection
  - Variable selection

**-- END TODO: WORK TO BE DONE. CLEAN THIS UP --**

# 1. Introduction: a study of housing market trends in Austin, Texas

The real estate market can be intimidating for anyone looking for a new home. For first time home buyers and experienced investors alike, news articles are citing that we're living through the hottest market in decades [^1]. 

For many people, buying a house is one of the most important decisions they'll make in their lives [^2]. However, when is the right time to buy a house? Are prices high right now, and will they further increase? Should someone consider purchasing a house thinking the market may worsen? How does one maximize their investment by buying at the right time? By predicting the home value based on what we know historically from houses in the same profile, we can help an individual understand if the price is inflated or not, and help them determine if this is the right moment. 

For this data analysis project, we will study the housing market in Austin, Texas, one of America's hottest real estate markets. By using historical data, we'll attempt to predict home prices using multiple linear regression, and apply methods discussed in class to find the best-possible prediction model.We believe that the trends in Austin can serve as a thermometer for the rest of the country due to high interest driven by the COVID-19 pandemic, which is shifting workers to home-offices [^3]. By using detailed historical data about properties that were sold in Austin in the past, our intent is to offer home buyers guidance to help them understand if the sale price for a house is within overall market expectation, helping them on the decision making process. 

This project implements the following data analysis techniques and concepts: 

- Data cleaning
- Collinearity
- Multiple linear regression
- ANOVA
- Interaction
- Assumption diagnostics
- Outlier diagnostics
- Transformations
- Stepwise model selection
- Variable selection

# 2. Dataset description and analysis 

We gathered data from a large study [^4] based on Zillow listings that include the sale price of properties between the years 2018 and 2021 in Austin, Texas, and several variables that help describe the given properties. We will try to predict prices in the past, present, and also future trends based on those variables. 

The data used in our study offers *15,171 observations* and *45 variables*. Following is the list of variables in the dataset. Except for `zpid`, every variable is either a predictor or a covariate.

1. `zpid`: Unique Identifier or Property ID
2. `city`: The lowercase name of a city or town in or surrounding Austin, Texas.
3. `streetaddress`: The street address of the property.
4. `zipcode`: The property's 5-digit ZIP code.
5. `description`: The description of the property listing from Zillow.
6. `latitude`: Latitude of the property.
7. `longitude`: Longitude of the property.
8. `propertyTaxRate`: Tax rate for the property.
9. `garageSpaces`: Number of garage spaces. This is a subset of the `ParkingSpaces` feature.
10. `hasAssociation`: Indicates if there is a Homeowners Association associated with the property.
11. `hasCooling`: Boolean indicating if the property has a cooling system.
12. `hasGarage`: Boolean indicating if the property has a garage.
13. `hasHeating`: Boolean indicating if the property has a heating system.
14. `hasSpa`: Boolean indicating if the property has a Spa.
15. `hasView`: Boolean indicating if the property comes with a view.
16. `homeType`: The home type (i.e., Single Family, Townhouse, Apartment).
17. `parkingSpaces`: The number of parking spots.
18. `yearBuilt`: The year the property was built.
19. `numPriceChanges`: The number of price changes the property has undergone since being listed.
20. `latest_saledate`: The latest sale date (YYYY-MM-DD).
21. `latest_salemonth`: The month the property sold (1-12).
22. `latest_saleyear`: The year the property sold (2018-2021). 
23. `latestPriceSource`: The party that provided the sale price.
24. `numOfPhotos`: The number of photos in the Zillow listing.
25. `numOfAccessibilities`: The number of unique accessibility features in the property.
26. `numOfAppliances`: The number of unique appliances in the property.
27. `numOfParkingFeatures`: The number of unique parking features in the property.
28. `numOfPatioAndPorts`: The number of unique patio and/or porch features in the property.
29. `numOfSecurityFeatures`: The number of unique security features in the property.
30. `numOfWaterFront`: The number of unique waterfront features in the property.
31. `numOfUniqueWindowFeatures`: The number of unique window aesthetics in the property. 
32. `numOfCommunityFeatures`: The number of unique community features (community meeting room, mailbox) in the property.
33. `lotSizeSqFt`: The lot size of the property reported in square feet.
34. `livingAreaSqFt`: The living area of the property reported in square feet.
35. `numOfPrimarySchools`: The number of primary schools listed in the area on the listing.
36. `numOfElementrySchools`: The number of elementary schools listed in the area on the listing.
37. `numOfMiddleSchools`: The number of middle schools listed in the area on the listing.
38. `numOfHighSchools`: The number of high schools listed in the area on the listing.
39. `avgSchoolDistance`: The average distance of all school types (i.e., Middle, High) in the listing.
40. `avgSchoolRating`: The average school rating of all school types (i.e., Middle, High) in the listing.
41. `avgSchoolSize`: The average school size of all school types (i.e., Middle, High) in the listing.
42. `MedianStudentsPerTeacher`: The median students-per-teacher for all schools near the listing.
43. `numOfBathrooms`: The number of bathrooms in the property.
44. `numOfBedrooms`: The number of bedrooms in the property.
45. `numOfStories`: The number of stories in the property.

## 2.1. Data cleaning

Our first step will be to read the data from a csv file (`austinHousingData.csv`) and perform some data cleaning tasks: 

- Remove rows with missing data
- The homeType variable has 10 different possible values (Apartment, Condo, Residential, etc). We'll make it a factor variable. 
- Remove variables that will not be used in the analysis for lack of relevancy to the property price: `zpid`, `latest_saledate`, `latestPriceSource`, `city`, `homeImage`, `streetAddress`, and `numOfPhotos` 

```{r, message=FALSE,echo=FALSE}
raw_housing_data = read.csv("austinHousingData.csv")

# remove all rows with missing data
raw_housing_data = na.omit(raw_housing_data)

# Make homeType a factor variable
raw_housing_data$homeType = as.factor(raw_housing_data$homeType)

# Remove predictors that are not used
selected_housing_data = subset(
  raw_housing_data,
  select = -c(
    zpid,
    latest_saledate,
    latestPriceSource,
    city,
    homeImage,
    streetAddress,
    numOfPhotos
  )
)
```

Now that we've performed basic data cleaning tasks, let's take a look at the dataset. 

```{r kable,message=FALSE,echo=FALSE}
head(selected_housing_data)
```

We'll also look at the summary of the dataset to better understand the data ranges for each variable. This allows us to understand if some of the variables present abnormal max or min values when compared to the mean of that variable, helping to identify outliers which may cause noise in the dataset. 

```{r}
summary(selected_housing_data)
```

Some of the variables demonstrate strange min and max values when compared to their mean: `avgSchoolDistance`, `livingAreaSqFt`, `lotSizeSqFt`,  `numOfBedrooms` and `numOfBathrooms`. We'll plot the observations of these variables in charts to better undertand if they are outliers. 

```{r}
dataVisuals = function(data) {
  par(mfrow = c(2, 3))
  
  plot(
    latestPrice ~ homeType,
    data = data,
    pch = 20,
    col = "dodgerblue",
    main = "latestPrice vs. homeType",
    cex = 1.5
  )
  plot(
    latestPrice ~ avgSchoolDistance  ,
    data = data,
    pch = 20,
    col = "dodgerblue",
    main = "latestPrice vs. avgSchoolDistance  ",
    cex = 1.5
  )
  plot(
    latestPrice ~ livingAreaSqFt,
    data = data,
    pch = 20,
    col = "dodgerblue",
    main = "latestPrice vs. livingAreaSqFt",
    cex = 1.5
  )
  
  plot(
    latestPrice ~ lotSizeSqFt,
    data = data,
    pch = 20,
    col = "dodgerblue",
    main = "latestPrice vs. lotSizeSqFt",
    cex = 1.5
  )
  plot(
    latestPrice ~ numOfBedrooms,
    data = data,
    pch = 20,
    col = "dodgerblue",
    main = "latestPrice vs. numOfBedrooms",
    cex = 1.5
  )
  plot(
    latestPrice ~ numOfBathrooms,
    data = data,
    pch = 20,
    col = "dodgerblue",
    main = "latestPrice vs. numOfBathrooms",
    cex = 1.5
  )
}

dataVisuals(selected_housing_data)
```

From the data structure and visuals, we see that there are significant outliers in the dataset. For instance, one observation has a `livingAreaSqft` value of *'109,292'*, compared to its mean *'1,975'*. Other similiar outliers are noticed in `lotSizeSqFt`, `numOfBedrooms`, and `numOfBathrooms`. We will remove these outliers using boxplot stats. 

```{r, warning=FALSE}
for (x in c('homeType','latestPrice', 'avgSchoolDistance', 'livingAreaSqFt', 'lotSizeSqFt', 'numOfBedrooms', 'numOfBathrooms'))
{
  value = selected_housing_data[,x][selected_housing_data[,x] %in% boxplot.stats(selected_housing_data[,x])$out]
  selected_housing_data[,x][selected_housing_data[,x] %in% value] = NA
} 

# remove all rows with missing data
selected_housing_data = na.omit(selected_housing_data)

```

Looking at the plots again, we confirm that the observations are better represented now, without outliers. 

```{r, warning=FALSE}
dataVisuals(selected_housing_data)
```

Now that we have a clean dataset, without outliers, let's have a look at the distribution of property prices when plotted over the map of Austin. We notice that the most expensive houses are concentrated near the central part of Austin, with some exceptions for more prestigious areas. Overal, houses are in the $250,000 to $750,000 range. 

```{r, warning=FALSE, message=FALSE}
library(ggmap)
library(ggplot2)

# register google maps API key
register_google(key = "AIzaSyAXuwivTHN6rIgi3teuusdz3r8dqNMQQx8")

## Central co-ordinates of the region we are interested in.
central_location = c(mean(selected_housing_data$longitude),
                     mean(selected_housing_data$latitude))

## Get map centered on Austin, TX (or the mean of the coordinates in our dataset)
austin_map = ggmap(get_googlemap(
  center = central_location,
  scale = 1,
  zoom = 10
),
extent = "normal")

## Plot heatmap
austin_map + geom_point(
  aes(x = longitude, y = latitude, color = latestPrice),
  data = selected_housing_data,
  alpha = 0.4,
  size = 3
) + xlim(range(selected_housing_data$longitude)) + ylim(range(selected_housing_data$latitude)) + scale_color_distiller(palette = "Spectral", labels = scales::comma) + xlab("Longitude") + ylab("Latitude") + ggtitle("Heatmap: latest sale price ($ USD) by property")
```

## 2.2. Train-Test split

Finally, we'll split our dataset into two data frames: one used for training, which will contain 25% of the observations, and one for testing, containing the remaining 75%.   

```{r}
set.seed(19870412)
ratio = 0.25
idx  = sample(nrow(selected_housing_data),
              size = nrow(selected_housing_data) * ratio)
housing_data_train = selected_housing_data[idx,]
housing_data_test = selected_housing_data[-idx,]
```

## 2.3. Collinearity

First, we'll look at the variables in the dataset to investigate if there's multicollinearity.

```{r}
library(faraway)
options(max.print=1000000)

# This is a helper function to get the top n items from a matrix. 
# Adjusted from https://stackoverflow.com/questions/32544566/find-the-largest-values-on-a-matrix-in-r

nlargest = function(m, n=10, sim = TRUE) {
  mult = 1;
  if (sim) mult = 2
  res = order(m, decreasing = TRUE)[seq_len(n) * mult]
  pos = arrayInd(res, dim(m), useNames = TRUE)
  list(values = m[res],
       position = pos)
}

# A correlation cannot be computed for factor variables.So we'll create a copy of the data frame without the factor variables to run the collinearity analysis
num_cols = unlist(lapply(housing_data_train, is.numeric))
housing_data_numerical = housing_data_train[ , num_cols]

# Pairs won't work with more than 26 variables
# pairs(housing_data_train[,1:26], col="dodgerblue")

# run cor() and store results on a matrix 
(coll_matrix = round(cor(housing_data_numerical),2))
```

```{r}
#garageSpaces is not always the same as parkingSpaces
spaces_different = housing_data_train$parkingSpaces != housing_data_train$garageSpaces

# Proportion of observations where parkingSpaces is different than garageSpaces
length(spaces_different[spaces_different==TRUE]) / length(spaces_different)
```

We don't observe examples of collinearity between variables above 0.8, except for the relationship between `parkingSpaces` and `garageSpaces`. Remember that `parkingSpaces` is the number of parking spots, while `garageSpaces` represents the number of garage spaces as a subset of the `ParkingSpaces` variable. The latest may include additional parking spaces provided by common areas. These variables contain the same value in 0.16% of the observations.

Therefore, we'll eliminate the `garageSpaces` variable from the dataset. 

```{r}
housing_data_train = subset(housing_data_train, select = -c(garageSpaces))
```

We'll further look for multicollinearity with the remaining variables. 

```{r}
num_cols = unlist(lapply(housing_data_train, is.numeric))
housing_data_numerical = housing_data_train[ , num_cols]
coll_matrix = round(cor(housing_data_numerical),2)
```

This matrix is extensive, and it may be easy to miss high values. So let's use a function to look at the highest values in the matrix. 

```{r}
# Look at the top values from coll_matrix that are different than 1: 
nlargest(coll_matrix, n=45)$values[nlargest(coll_matrix, n=45)$values < 1]
```

We now see that there's no collinearity between variables that's higher than 0.8. Still, we can further investigate the model to see if there's any variable impacting the response at considerable rates when compared to the others. 


```{r}
library('faraway')
housing_data_model = lm(latestPrice ~ ., data = housing_data_train)

vif = vif(housing_data_model)
vif[which(vif > 5)]

```

Variable `numOfPrimarySchools` shows a VIF greater than 5, which may be a concern. What proportion of the observed variation in latestPrice is explained by a linear relationship with `numOfPrimarySchools`? How about `parkingSpaces`?

```{r}

summary(lm(hasGarage ~. -latestPrice, data = housing_data_train))$r.squared

housing_data_model_non_significant = lm(latestPrice ~ .-hasGarage, data = housing_data_train)
vif_non_significant = vif(housing_data_model_non_significant)
vif_non_significant[which(vif_non_significant > 5)]

#Finally, compare both models
(anova_results = anova(housing_data_model, housing_data_model_non_significant))

# As p-value is significant (0.94), we fail to reject and consider remove hasGarage predictor.
housing_data_train = subset(housing_data_train, select = -c(hasGarage))

```
# 3. Model Buidling

## 3.1 Additive model

```{r warning=FALSE}

housing_data_model = lm(latestPrice ~ ., data = housing_data_train)

length(coef(housing_data_model))
## 43 predictors

summary(housing_data_model)$r.squared # 0.5569
summary(housing_data_model)$adj.r.squared # 0.5509

# sqrt(mean((resid(housing_data_model) / (1 - hatvalues(housing_data_model))) ^ 2)) 

```

From the R2 value about 53.9% data is explained by this model and there are 46 predictors in the model. Next, we will try to find a “better” model with higher R2 (> 0.5392) or adjusted R2 (> 0.5371) and lower value of LOOCV_RMSE (< 111435) to explain the data.

Interaction model has better adjusted R squared value. 0.7132 compared to additive model 0.4134

```{r warning=FALSE}

## Additive model AIC and BIC

housing_data_model_aic = step(housing_data_model, direction = "backward", trace = 0)
extractAIC(housing_data_model_aic) # returns both p and AIC
## 29 predictors
summary(housing_data_model_aic)$adj.r.squared
## [1] 0.5515

housing_data_model_bic = step(housing_data_model, direction = "backward", trace = 0, k = log(nrow(housing_data_numerical)))
extractAIC(housing_data_model_bic)  # returns both p and AIC
## 20 predictors
summary(housing_data_model_bic)$adj.r.squared
## [1] 0.5467


## Exhaustive Search

library(leaps)

housing_data_model_leaps = summary(regsubsets(latestPrice ~ ., data = housing_data_train))

housing_data_model_leaps$rss

housing_data_model_leaps$adjr2

housing_data_model_leaps_r2_index = which.max(housing_data_model_leaps$adjr2)  # 0.5167

housing_data_model_leaps$which[housing_data_model_leaps_r2_index, ]

housing_data_model_leaps_best = lm(latestPrice ~ zipcode + propertyTaxRate + hasAssociation + latest_saleyear + yearBuilt + numPriceChanges + numOfWaterfrontFeatures +  avgSchoolSize + 
                                     livingAreaSqFt + avgSchoolRating , data = housing_data_train )

summary(housing_data_model_leaps_best)

anova(housing_data_model, housing_data_model_leaps_best) # p-value is <2e-16 # Reject null hypothesis. 
# leaps best model can be considered 

```

## 3.2. Interactive model

```{r}

housing_data_model_interaction = lm(latestPrice ~ (zipcode + propertyTaxRate + hasAssociation + yearBuilt + 
    numPriceChanges + numOfWaterfrontFeatures + avgSchoolSize + livingAreaSqFt + latest_saleyear + avgSchoolRating)^2, data = housing_data_train)

summary(housing_data_model_interaction)

length(coef(housing_data_model_interaction))## 56 predictors

summary(housing_data_model_interaction)$r.squared # 0.5899
summary(housing_data_model_interaction)$adj.r.squared # 0.5836

anova(housing_data_model, housing_data_model_interaction)[2,"Pr(>F)"] 
# 2.438e-49
# Reject Null hypothesis

```

## 3.3. Model Selection on interactions

```{r}

housing_data_model_interaction_aic = step(housing_data_model_interaction, direction = "backward", trace = 0)
extractAIC(housing_data_model_interaction_aic) # returns both p and AIC
housing_data_model_interaction_aic
# 31 predictors  72174
summary(housing_data_model_interaction_aic)$adj.r.squared
# [1] 0.5844

housing_data_model_interaction_bic = step(housing_data_model_interaction, direction = "backward", trace = 0, k = log(nrow(housing_data_numerical)))

extractAIC(housing_data_model_interaction_bic)  # returns both p and AIC
# 25  predictors 72189
summary(housing_data_model_interaction_bic)$adj.r.squared
# [1] 0.581

```

## 3.4. Diagnostics

```{r}

diagnostics = function (model){
  
par(mfrow = c(1, 3))

plot(fitted(model), resid(model), pch = 20,
                xlab = "Fitted Values",
                ylab = "Residuals",
                main = "Fitted vs Residuals",
                col = "grey")

abline(h = 0, lwd = 2, col = "orange")
        
qqnorm(resid(model), pch = 20, main = "QQNorm Plot",col = "grey")
qqline( resid(model),lwd = 2, col =  "orange")

hist(resid(model),main = "Histogram of Residuals",col = "orange",xlab = "Residuals",ylab = "Frequency")

library(lmtest)
bptest(model)
shapiro.test(resid(model))

}

diagnostics(housing_data_model_interaction_aic)

```

## 3.5. Outliers
```{r}

cooksd = cooks.distance(housing_data_model_interaction_aic)

plot(cooksd, pch="*", cex=2, main="Influential Observations by Cooks distance")  # plot cook's distance
abline(h = 2*mean(cooksd, na.rm=T), col="black")  # add cutoff line

text(x=1:length(cooksd)+1, y=cooksd, labels=ifelse(cooksd>2*mean(cooksd, na.rm=T),names(cooksd),""), col="red")  # add labels



housing_data_model_interaction_aic_without_outliers = lm(latestPrice ~ zipcode + propertyTaxRate + hasAssociation + 
    yearBuilt + numPriceChanges + numOfWaterfrontFeatures + avgSchoolSize + 
    livingAreaSqFt + latest_saleyear + avgSchoolRating + zipcode:numPriceChanges + 
    zipcode:numOfWaterfrontFeatures + zipcode:avgSchoolSize + 
    zipcode:livingAreaSqFt + propertyTaxRate:hasAssociation + 
    propertyTaxRate:yearBuilt + propertyTaxRate:avgSchoolSize + 
    propertyTaxRate:livingAreaSqFt + propertyTaxRate:avgSchoolRating + 
    hasAssociation:yearBuilt + hasAssociation:numPriceChanges + 
    hasAssociation:avgSchoolSize + hasAssociation:livingAreaSqFt + 
    hasAssociation:latest_saleyear + hasAssociation:avgSchoolRating + 
    yearBuilt:numOfWaterfrontFeatures + yearBuilt:avgSchoolSize + 
    yearBuilt:latest_saleyear + yearBuilt:avgSchoolRating + numPriceChanges:livingAreaSqFt + 
    numPriceChanges:latest_saleyear + avgSchoolSize:livingAreaSqFt + 
    avgSchoolSize:latest_saleyear + avgSchoolSize:avgSchoolRating + 
    livingAreaSqFt:avgSchoolRating,  data = housing_data_train,  subset = cooksd < 2*mean(cooksd, na.rm=T))

diagnostics(housing_data_model_interaction_aic_without_outliers)
bptest(housing_data_model_interaction_aic_without_outliers)
 
```
```{r}
library(MASS)
boxcox(housing_data_model_interaction_aic_without_outliers, plotit = TRUE, lambda = seq(0, 1, by = 0.05))

housing_data_model_interaction_aic_without_outliers = lm((((latestPrice ^ 0.5) -1)/0.5) ~ zipcode + propertyTaxRate + hasAssociation + 
    yearBuilt + numPriceChanges + numOfWaterfrontFeatures + avgSchoolSize + 
    livingAreaSqFt + latest_saleyear + avgSchoolRating + zipcode:numPriceChanges + 
    zipcode:numOfWaterfrontFeatures + zipcode:avgSchoolSize + 
    zipcode:livingAreaSqFt + propertyTaxRate:hasAssociation + 
    propertyTaxRate:yearBuilt + propertyTaxRate:avgSchoolSize + 
    propertyTaxRate:livingAreaSqFt + propertyTaxRate:avgSchoolRating + 
    hasAssociation:yearBuilt + hasAssociation:numPriceChanges + 
    hasAssociation:avgSchoolSize + hasAssociation:livingAreaSqFt + 
    hasAssociation:latest_saleyear + hasAssociation:avgSchoolRating + 
    yearBuilt:numOfWaterfrontFeatures + yearBuilt:avgSchoolSize + 
    yearBuilt:latest_saleyear + yearBuilt:avgSchoolRating + numPriceChanges:livingAreaSqFt + 
    numPriceChanges:latest_saleyear + avgSchoolSize:livingAreaSqFt + 
    avgSchoolSize:latest_saleyear + avgSchoolSize:avgSchoolRating + 
    livingAreaSqFt:avgSchoolRating,  data = housing_data_train,  subset = cooksd < 2*mean(cooksd, na.rm=T))

diagnostics(housing_data_model_interaction_aic_without_outliers)
bptest(housing_data_model_interaction_aic_without_outliers)

```
 
<br/> We used Box Cox transformation on our model to improve constant variance. As visible in box cox we can see that normal distribution mean is centered around 0.43. We tried using 0.43 to 0.50 as exponential formula as per box cox transformation and we got best model at 0.50. Our p-value for normal distribution is 0.4 howevere constant variation is still haveing low p-value. As we improved a lot based on fitted vs residual plot so we are making this as our final model after applying box cox transofrmtion.
<br/>
# Links and citations

## Appendix A: libraries used

The following libraries were used in the creation of this report: 

**- ggmap**
  
    D. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2.
    
    The R Journal, 5(1), 144-161. URL
    
    http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf
    
**- ggplot2**
  
    H. Wickham. ggplot2: Elegant Graphics for Data Analysis.
    
    Springer-Verlag New York, 2016.
  
**- readr**

**- faraway**
  
    Julian Faraway (2016). faraway: Functions and Datasets for Books
  
    by Julian Faraway. R package version 1.0.7.
  
    https://CRAN.R-project.org/package=faraway
  
**- lmtest**
  
    Achim Zeileis, Torsten Hothorn (2002). Diagnostic Checking in
    
    Regression Relationships. R News 2(3), 7-10. URL
    
    https://CRAN.R-project.org/doc/Rnews/
  
## Appendix B: links

[^1]:"How To Succeed As A First-Time Home Buyer In Today’s Market" (https://www.forbes.com/sites/forbesrealestatecouncil/2021/07/19/how-to-succeed-as-a-first-time-home-buyer-in-todays-market/?sh=79e0d37f19f8)
[^2]:"Your 4 Most Important Financial Decisions: #1 – The House Purchase" (https://www.retirementstewardship.com/2016/05/28/4-important-financial-decisions-1-house-purchase/)
[^3]:"Why hot-desking is a terrible idea" (https://www.msn.com/en-us/lifestyle/career/why-hot-desking-is-a-terrible-idea/ar-AAMjgTM?ocid=BingNewsSearch)
[^4]: [Kaggle](https://www.kaggle.com/) dataset: "Austin, TX House Listings - Features and Images scraped in January 2021". (https://www.kaggle.com/ericpierce/austinhousingprices, `austinHousingData.csv`).

# Links and citations

## Appendix A: libraries used

The following libraries were used in the creation of this report: 

**- ggmap**
  
    D. Kahle and H. Wickham. ggmap: Spatial Visualization with ggplot2.
    
    The R Journal, 5(1), 144-161. URL
    
    http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf
    
**- ggplot2**
  
    H. Wickham. ggplot2: Elegant Graphics for Data Analysis.
    
    Springer-Verlag New York, 2016.
  
**- readr**

**- faraway**
  
    Julian Faraway (2016). faraway: Functions and Datasets for Books
  
    by Julian Faraway. R package version 1.0.7.
  
    https://CRAN.R-project.org/package=faraway
  
**- lmtest**
  
    Achim Zeileis, Torsten Hothorn (2002). Diagnostic Checking in
    
    Regression Relationships. R News 2(3), 7-10. URL
    
    https://CRAN.R-project.org/doc/Rnews/
  
## Appendix B: links

[^1]:"How To Succeed As A First-Time Home Buyer In Today’s Market" (https://www.forbes.com/sites/forbesrealestatecouncil/2021/07/19/how-to-succeed-as-a-first-time-home-buyer-in-todays-market/?sh=79e0d37f19f8)
[^2]:"Your 4 Most Important Financial Decisions: #1 – The House Purchase" (https://www.retirementstewardship.com/2016/05/28/4-important-financial-decisions-1-house-purchase/)
[^3]:"Why hot-desking is a terrible idea" (https://www.msn.com/en-us/lifestyle/career/why-hot-desking-is-a-terrible-idea/ar-AAMjgTM?ocid=BingNewsSearch)
[^4]: [Kaggle](https://www.kaggle.com/) dataset: "Austin, TX House Listings - Features and Images scraped in January 2021". (https://www.kaggle.com/ericpierce/austinhousingprices, `austinHousingData.csv`).

